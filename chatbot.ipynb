{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these dependencies to install\n",
    "# !pip install langchain==0.0.263\n",
    "# !pip install openai==0.27.2\n",
    "# !pip install redis==4.6.0\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install gdown\n",
    "# !pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "MAX_TEXT_LENGTH=1000  # Maximum num of text characters to use\n",
    " \n",
    "def auto_truncate(val):\n",
    " \n",
    "    \"\"\"Truncate the given text.\"\"\"\n",
    " \n",
    "    return val[:MAX_TEXT_LENGTH]\n",
    "    \n",
    "# Load Product data and truncate long text fields\n",
    " \n",
    "all_prods_df = pd.read_csv(\"flipkart_com-ecommerce_sample.csv\", converters={\n",
    " \n",
    "    'description': auto_truncate,\n",
    " \n",
    "    'product_specifications': auto_truncate,\n",
    " \n",
    "    'product_name': auto_truncate,\n",
    "    \n",
    "    'product_category_tree': auto_truncate,\n",
    " \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with None and drop\n",
    " \n",
    "all_prods_df['product_specifications'].replace('', None, inplace=True)\n",
    " \n",
    "all_prods_df.dropna(subset=['product_specifications'], inplace=True)\n",
    " \n",
    "# Reset pandas dataframe index\n",
    " \n",
    "all_prods_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num products to use (subset)\n",
    "NUMBER_PRODUCTS = 1\n",
    " \n",
    "# Get the first 2500 products\n",
    "product_metadata = ( \n",
    "    all_prods_df\n",
    "     .head(NUMBER_PRODUCTS)\n",
    "     .to_dict(orient='index')\n",
    ")\n",
    " \n",
    "# Check one of the products\n",
    "product_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.redis import Redis as RedisVectorStore\n",
    " \n",
    "# set your openAI api key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    " \n",
    "# data that will be embedded and converted to vectors\n",
    "texts = [\n",
    "    v['product_name'] for k, v in product_metadata.items()\n",
    "]\n",
    "\n",
    "# product metadata that we'll store along our vectors\n",
    "metadatas = list(product_metadata.values())\n",
    " \n",
    "# we will use OpenAI as our embeddings provider\n",
    "embedding = OpenAIEmbeddings()\n",
    " \n",
    "# name of the Redis search index to create\n",
    "index_name = \"products\"\n",
    " \n",
    "# assumes you have a redis stack server running on local host\n",
    "redis_url = \"redis://localhost:6379\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = RedisVectorStore.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding,\n",
    "    metadatas=metadatas,\n",
    "    index_name=index_name,\n",
    "    redis_url=redis_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain,\n",
    "    LLMChain\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following chat history and a follow up question, rephrase the follow up input question to be a standalone question.\n",
    "Or end the conversation if it seems like it's done.\n",
    "\n",
    "Chat History:\\\"\"\"\n",
    "{chat_history}\n",
    "\\\"\"\"\n",
    "\n",
    "Follow Up Input: \\\"\"\"\n",
    "{question}\n",
    "\\\"\"\"\n",
    "\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "condense_question_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "template = \"\"\"You are a friendly, conversational retail shopping assistant. Use the following context including product names, descriptions, image and product URL's to show the shopper whats available, help find what they want, and answer any questions.\n",
    "It's ok if you don't know the answer, also give reasons for recommending the product which you are about to suggest the customer. Always return the product URL and a single image url of the products you are recommending to the customers.\n",
    "\n",
    "Context:\\\"\"\"\n",
    "{context}\n",
    "\\\"\"\"\n",
    "\n",
    "Question:\\\"\n",
    "\\\"\"\"\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "qa_prompt= PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# define two LLM models from OpenAI\n",
    "llm = OpenAI(temperature=0.3)\n",
    "\n",
    "streaming_llm = OpenAI(\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    temperature=0.3,\n",
    "    max_tokens=8000\n",
    ")\n",
    "\n",
    "# use the LLM Chain to create a question creation chain\n",
    "question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=condense_question_prompt\n",
    ")\n",
    "\n",
    "# use the streaming LLM to create a question answering chain\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=streaming_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=qa_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel\n",
    " \n",
    "class RedisProductRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: VectorStore\n",
    " \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    " \n",
    "    def combine_metadata(self, doc) -> str:\n",
    "        metadata = doc.metadata\n",
    "        return (\n",
    "           \"Product Name: \" + metadata[\"product_name\"] + \". \" +\n",
    "           \"Product Description: \" + metadata[\"description\"] + \". \" +\n",
    "           \"Product URL: \" + metadata[\"product_url\"] + \".\" +\n",
    "           \"image: \" + metadata[\"image\"] + \".\"\n",
    "        )\n",
    " \n",
    "    def get_relevant_documents(self, query):\n",
    "        docs = []\n",
    "        for doc in self.vectorstore.similarity_search(query):\n",
    "            content = self.combine_metadata(doc)\n",
    "            docs.append(Document(\n",
    "                page_content=content,\n",
    "                metadata=doc.metadata\n",
    "            ))\n",
    " \n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_product_retriever = RedisProductRetriever(vectorstore=vectorstore)\n",
    " \n",
    "chatbot = ConversationalRetrievalChain(\n",
    "    retriever=redis_product_retriever,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chat history buffer\n",
    "chat_history = []\n",
    "# gather user input for the first question to kick off the bot\n",
    "question = input(\"Hi! What are you looking for today?\")\n",
    "# keep the bot running in a loop to simulate a conversation\n",
    "while True:\n",
    "    result = chatbot(\n",
    "        {\"question\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    print(\"user:\", result[\"question\"])\n",
    "    print(\"bot:\", result[\"answer\"])\n",
    "    print(\"\\n\")\n",
    "    chat_history.append((result[\"question\"], result[\"answer\"]))\n",
    "    question = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"ChatGPT-like clone\")\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "if \"openai_model\" not in st.session_state:\n",
    "    st.session_state[\"openai_model\"] = \"gpt-3.5-turbo\"\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "        for response in openai.ChatCompletion.create(\n",
    "            model=st.session_state[\"openai_model\"],\n",
    "            messages=[\n",
    "                {\"role\": m[\"role\"], \"content\": m[\"content\"]}\n",
    "                for m in st.session_state.messages\n",
    "            ],\n",
    "            stream=True,\n",
    "        ):\n",
    "            full_response += response.choices[0].delta.get(\"content\", \"\")\n",
    "            message_placeholder.markdown(full_response + \"▌\")\n",
    "        message_placeholder.markdown(full_response)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
