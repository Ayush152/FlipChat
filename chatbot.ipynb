{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.155\n",
    "!pip install openai==0.27.2\n",
    "!pip install redis==4.6.0\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install gdown\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "MAX_TEXT_LENGTH=1000  # Maximum num of text characters to use\n",
    " \n",
    "def auto_truncate(val):\n",
    " \n",
    "    \"\"\"Truncate the given text.\"\"\"\n",
    " \n",
    "    return val[:MAX_TEXT_LENGTH]\n",
    "    \n",
    "# Load Product data and truncate long text fields\n",
    " \n",
    "all_prods_df = pd.read_csv(\"flipkart_com-ecommerce_sample.csv\", converters={\n",
    " \n",
    "    'description': auto_truncate,\n",
    " \n",
    "    'product_specifications': auto_truncate,\n",
    " \n",
    "    'product_name': auto_truncate,\n",
    "    \n",
    "    'product_category_tree': auto_truncate,\n",
    " \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings with None and drop\n",
    " \n",
    "all_prods_df['product_specifications'].replace('', None, inplace=True)\n",
    " \n",
    "all_prods_df.dropna(subset=['product_specifications'], inplace=True)\n",
    " \n",
    "# Reset pandas dataframe index\n",
    " \n",
    "all_prods_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num products to use (subset)\n",
    "NUMBER_PRODUCTS = 500\n",
    " \n",
    "# Get the first 2500 products\n",
    "product_metadata = ( \n",
    "    all_prods_df\n",
    "     .head(NUMBER_PRODUCTS)\n",
    "     .to_dict(orient='index')\n",
    ")\n",
    " \n",
    "# Check one of the products\n",
    "product_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.redis import Redis as RedisVectorStore\n",
    " \n",
    "# set your openAI api key as an environment variable\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    " \n",
    "# data that will be embedded and converted to vectors\n",
    "texts = [\n",
    "    v['product_name'] for k, v in product_metadata.items()\n",
    "]\n",
    "\n",
    "# product metadata that we'll store along our vectors\n",
    "metadatas = list(product_metadata.values())\n",
    " \n",
    "# we will use OpenAI as our embeddings provider\n",
    "embedding = OpenAIEmbeddings()\n",
    " \n",
    "# name of the Redis search index to create\n",
    "index_name = \"products\"\n",
    " \n",
    "# assumes you have a redis stack server running on local host\n",
    "redis_url = \"redis://default:root@redis-17102.c305.ap-south-1-1.ec2.cloud.redislabs.com:17102\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = RedisVectorStore.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding,\n",
    "    metadatas=metadatas,\n",
    "    index_name=index_name,\n",
    "    redis_url=redis_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain,\n",
    "    LLMChain\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "template = \"\"\"Given the following chat history and a follow up question, rephrase the follow up input question to be a standalone question.\n",
    "Or end the conversation if it seems like it's done.\n",
    "\n",
    "Chat History:\\\"\"\"\n",
    "{chat_history}\n",
    "\\\"\"\"\n",
    "\n",
    "Follow Up Input: \\\"\"\"\n",
    "{question}\n",
    "\\\"\"\"\n",
    "\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "condense_question_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "template = \"\"\"You are a friendly, conversational retail shopping assistant. Use the following context including product names, descriptions, and keywords to show the shopper whats available, help find what they want, and answer any questions.\n",
    "It's ok if you don't know the answer.\n",
    "\n",
    "Context:\\\"\"\"\n",
    "{context}\n",
    "\\\"\"\"\n",
    "\n",
    "Question:\\\"\n",
    "\\\"\"\"\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "qa_prompt= PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# define two LLM models from OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "streaming_llm = OpenAI(\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    temperature=0.2,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# use the LLM Chain to create a question creation chain\n",
    "question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=condense_question_prompt\n",
    ")\n",
    "\n",
    "# use the streaming LLM to create a question answering chain\n",
    "doc_chain = load_qa_chain(\n",
    "    llm=streaming_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=qa_prompt\n",
    ")\n",
    "\n",
    "\n",
    "chatbot = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator\n",
    ")\n",
    "# create a chat history buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chat history buffer\n",
    "chat_history = []\n",
    "# gather user input for the first question to kick off the bot\n",
    "question = input(\"Hi! What are you looking for today?\")\n",
    "# keep the bot running in a loop to simulate a conversation\n",
    "while True:\n",
    "    result = chatbot(\n",
    "        {\"question\": question, \"chat_history\": chat_history}\n",
    "    )\n",
    "    print(\"user:\", result[\"question\"])\n",
    "    print(\"bot:\", result[\"answer\"])\n",
    "    print(\"\\n\")\n",
    "    chat_history.append((result[\"question\"], result[\"answer\"]))\n",
    "    question = input()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
